{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a62249c4-446f-461b-8931-48586b9882de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parts of the U-Net model \"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class DoubleConv(nn.Module):\n",
    "#     \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "#\n",
    "#     def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "#         super().__init__()\n",
    "#         if not mid_channels:\n",
    "#             mid_channels = out_channels\n",
    "#         self.double_conv = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(mid_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         return self.double_conv(x)\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2 in 3D\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "\n",
    "# class Down(nn.Module):\n",
    "#     \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "#\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super().__init__()\n",
    "#         self.maxpool_conv = nn.Sequential(\n",
    "#             nn.MaxPool2d(2),\n",
    "#             DoubleConv(in_channels, out_channels)\n",
    "#         )\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         return self.maxpool_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv in 3D\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool3d(2),\n",
    "            DoubleConv(in_channels, out_channels)  # Assuming DoubleConv3D is defined as per previous conversation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "# class Up(nn.Module):\n",
    "#     \"\"\"Upscaling then double conv\"\"\"\n",
    "#\n",
    "#     def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "#         super().__init__()\n",
    "#\n",
    "#         # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "#         if bilinear:\n",
    "#             self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "#             self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "#         else:\n",
    "#             self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "#             self.conv = DoubleConv(in_channels, out_channels)\n",
    "#\n",
    "#\n",
    "#     def forward(self, x1, x2):\n",
    "#         x1 = self.up(x1)\n",
    "#         # input is CHW\n",
    "#         diffY = x2.size()[2] - x1.size()[2]\n",
    "#         diffX = x2.size()[3] - x1.size()[3]\n",
    "#\n",
    "#         x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "#                         diffY // 2, diffY - diffY // 2])\n",
    "#         # if you have padding issues, see\n",
    "#         # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "#         # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "#         x = torch.cat([x2, x1], dim=1)\n",
    "#         return self.conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv in 3D\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose3d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # Adjust for 3D\n",
    "        diffZ = x2.size()[2] - x1.size()[2]\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        diffX = x2.size()[4] - x1.size()[4]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2, diffZ // 2, diffZ - diffZ // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "580744b6-3712-4d91-82ee-4ee8a6bf1f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# class PAM_Module(nn.Module):\n",
    "#     \"\"\"空间注意力模块\"\"\"\n",
    "#     def __init__(self, in_dim):\n",
    "#         super(PAM_Module, self).__init__()\n",
    "#         self.chanel_in = in_dim\n",
    "#\n",
    "#         self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
    "#         self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
    "#         self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "#         self.gamma = nn.Parameter(torch.zeros(1))\n",
    "#         self.softmax = nn.Softmax(dim=-1)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         m_batchsize, C, height, width = x.size()\n",
    "#         proj_query = self.query_conv(x).view(m_batchsize, -1, width * height).permute(0, 2, 1)\n",
    "#         proj_key = self.key_conv(x).view(m_batchsize, -1, width * height)\n",
    "#\n",
    "#         energy = torch.bmm(proj_query, proj_key)\n",
    "#         attention = self.softmax(energy)\n",
    "#         proj_value = self.value_conv(x).view(m_batchsize, -1, width * height)\n",
    "#\n",
    "#         out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "#         out = out.view(m_batchsize, C, height, width)\n",
    "#\n",
    "#         out = self.gamma * out + x\n",
    "#         return out\n",
    "class PAM_Module(nn.Module):\n",
    "    \"\"\"Spatial Attention Module for 3D data\"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super(PAM_Module, self).__init__()\n",
    "        self.channel_in = in_dim\n",
    "\n",
    "        self.query_conv = nn.Conv3d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv3d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv3d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        m_batchsize, C, depth, height, width = x.size()\n",
    "        proj_query = self.query_conv(x).view(m_batchsize, -1, depth * height * width).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(m_batchsize, -1, depth * height * width)\n",
    "\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = self.softmax(energy)\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, depth * height * width)\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(m_batchsize, C, depth, height, width)\n",
    "\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "# class PositionEmbeddingLearned(nn.Module):\n",
    "#     \"\"\"\n",
    "#     可学习的位置编码\n",
    "#     \"\"\"\n",
    "#     def __init__(self, num_pos_feats=256, len_embedding=32):\n",
    "#         super().__init__()\n",
    "#         self.row_embed = nn.Embedding(len_embedding, num_pos_feats)\n",
    "#         self.col_embed = nn.Embedding(len_embedding, num_pos_feats)\n",
    "#         self.reset_parameters()\n",
    "#\n",
    "#     def reset_parameters(self):\n",
    "#         nn.init.uniform_(self.row_embed.weight)\n",
    "#         nn.init.uniform_(self.col_embed.weight)\n",
    "#\n",
    "#     def forward(self, tensor_list):\n",
    "#         x = tensor_list\n",
    "#         h, w = x.shape[-2:]\n",
    "#         i = torch.arange(w, device=x.device)\n",
    "#         j = torch.arange(h, device=x.device)\n",
    "#\n",
    "#         x_emb = self.col_embed(i)\n",
    "#         y_emb = self.row_embed(j)\n",
    "#\n",
    "#         pos = torch.cat([\n",
    "#             x_emb.unsqueeze(0).repeat(h, 1, 1),\n",
    "#             y_emb.unsqueeze(1).repeat(1, w, 1),\n",
    "#         ], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n",
    "#\n",
    "#         return pos\n",
    "class PositionEmbeddingLearned(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable position encoding for 3D data\n",
    "    \"\"\"\n",
    "    def __init__(self, num_pos_feats=512, len_embedding=50):\n",
    "        super().__init__()\n",
    "        # Divide the total positional features equally among depth, height, and width\n",
    "        # self.feature_per_dim = num_pos_feats // 3\n",
    "        self.row_embed = nn.Embedding(len_embedding, 171)\n",
    "        self.col_embed = nn.Embedding(len_embedding, 171)\n",
    "        self.depth_embed = nn.Embedding(len_embedding, 170)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.row_embed.weight)\n",
    "        nn.init.uniform_(self.col_embed.weight)\n",
    "        nn.init.uniform_(self.depth_embed.weight)\n",
    "\n",
    "    def forward(self, tensor_list):\n",
    "        x = tensor_list\n",
    "        print(x.shape)\n",
    "        d, h, w = x.shape[-3:]\n",
    "        i = torch.arange(w, device=x.device)\n",
    "        j = torch.arange(h, device=x.device)\n",
    "        k = torch.arange(d, device=x.device)\n",
    "\n",
    "        x_emb = self.col_embed(i)\n",
    "        print(x_emb.shape)\n",
    "        y_emb = self.row_embed(j)\n",
    "        print(y_emb.shape)\n",
    "        z_emb = self.depth_embed(k)\n",
    "        print(z_emb.shape)\n",
    "\n",
    "        pos = torch.cat([\n",
    "            x_emb.unsqueeze(0).unsqueeze(0).repeat(d, h, 1, 1),\n",
    "            y_emb.unsqueeze(1).unsqueeze(0).repeat(d, 1, w, 1),\n",
    "            z_emb.unsqueeze(1).unsqueeze(1).repeat(1, h, w, 1)\n",
    "        ], dim=-1)\n",
    "        print(pos.shape)\n",
    "        pos = pos.permute(3, 0, 1, 2).unsqueeze(0).repeat(x.shape[0], 1, 1, 1, 1)\n",
    "        return pos\n",
    "\n",
    "\n",
    "# class ScaledDotProductAttention(nn.Module):\n",
    "#     '''自注意力模块'''\n",
    "#\n",
    "#     def __init__(self, temperature, attn_dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.temperature = temperature ** 0.5\n",
    "#         self.dropout = nn.Dropout(attn_dropout)\n",
    "#\n",
    "#     def forward(self, x, mask=None):\n",
    "#         m_batchsize, d, height, width = x.size()\n",
    "#         q = x.view(m_batchsize, d, -1)\n",
    "#         k = x.view(m_batchsize, d, -1)\n",
    "#         k = k.permute(0, 2, 1)\n",
    "#         v = x.view(m_batchsize, d, -1)\n",
    "#\n",
    "#         attn = torch.matmul(q / self.temperature, k)\n",
    "#\n",
    "#         if mask is not None:\n",
    "#             # 给需要mask的地方设置一个负无穷\n",
    "#             attn = attn.masked_fill(mask == 0, -1e9)\n",
    "#\n",
    "#         attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "#         output = torch.matmul(attn, v)\n",
    "#         output = output.view(m_batchsize, d, height, width)\n",
    "#\n",
    "#         return output\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    '''Self-attention module for 3D data'''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature ** 0.5\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        m_batchsize, d, depth, height, width = x.size()\n",
    "        q = x.view(m_batchsize, d, -1)\n",
    "        k = x.view(m_batchsize, d, -1)\n",
    "        k = k.permute(0, 2, 1)\n",
    "        v = x.view(m_batchsize, d, -1)\n",
    "\n",
    "        attn = torch.matmul(q / self.temperature, k)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        output = torch.matmul(attn, v)\n",
    "        output = output.view(m_batchsize, d, depth, height, width)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# class DoubleConv(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "#         super().__init__()\n",
    "#         if not mid_channels:\n",
    "#             mid_channels = out_channels\n",
    "#         self.double_conv = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(mid_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         return self.double_conv(x)\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75077fb5-10fe-42a8-bb43-a2245e12b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "'''维度转换'''\n",
    "# class MultiConv(nn.Module):\n",
    "#     def __init__(self, in_ch, out_ch, attn=True):\n",
    "#         super(MultiConv, self).__init__()\n",
    "#\n",
    "#         self.fuse_attn = nn.Sequential(\n",
    "#             nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(out_ch),\n",
    "#             nn.PReLU(),\n",
    "#             nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(out_ch),\n",
    "#             nn.PReLU(),\n",
    "#             nn.Conv2d(out_ch, out_ch, kernel_size=1),\n",
    "#             nn.BatchNorm2d(out_ch),\n",
    "#             nn.Softmax2d() if attn else nn.PReLU()\n",
    "#         )\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         return self.fuse_attn(x)\n",
    "class MultiConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, attn=True):\n",
    "        super(MultiConv, self).__init__()\n",
    "\n",
    "        self.fuse_attn = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_ch),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv3d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_ch),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv3d(out_ch, out_ch, kernel_size=1),\n",
    "            nn.BatchNorm3d(out_ch),\n",
    "            nn.Softmax(dim=1) if attn else nn.PReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fuse_attn(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5117dfe2-eaac-49df-be69-f54583073f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 8, 8, 8])\n",
      "torch.Size([8, 171])\n",
      "torch.Size([8, 171])\n",
      "torch.Size([8, 170])\n",
      "torch.Size([8, 8, 8, 512])\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Got 5D input, but bilinear mode needs 4D input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m))\n\u001b[1;32m     74\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet_Attention_Transformer_Multiscale(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, bilinear\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 75\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36mUNet_Attention_Transformer_Multiscale.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m x5 \u001b[38;5;241m=\u001b[39m x5_sdpa \u001b[38;5;241m+\u001b[39m x5_pam\n\u001b[1;32m     52\u001b[0m x6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup1(x5, x4)\n\u001b[0;32m---> 53\u001b[0m x5_scale \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx6\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbilinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m x6_cat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x5_scale, x6), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     56\u001b[0m x7 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2(x6_cat, x3)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py:4041\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4039\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot 5D input, but linear mode needs 3D input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 4041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot 5D input, but bilinear mode needs 4D input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4043\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   4044\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Error: Only 3D, 4D and 5D input Tensors supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4045\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4046\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4047\u001b[0m )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Got 5D input, but bilinear mode needs 4D input"
     ]
    }
   ],
   "source": [
    "class UNet_Attention_Transformer_Multiscale(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet_Attention_Transformer_Multiscale, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(1024, 256 // factor, bilinear)\n",
    "        self.up3 = Up(512, 128 // factor, bilinear)\n",
    "        self.up4 = Up(256, 64, bilinear)\n",
    "        self.outc = OutConv(128, n_classes)\n",
    "\n",
    "        '''位置编码'''\n",
    "        self.pos = PositionEmbeddingLearned(512 // factor)\n",
    "\n",
    "        '''空间注意力机制'''\n",
    "        self.pam = PAM_Module(512)\n",
    "\n",
    "        '''自注意力机制'''\n",
    "        self.sdpa = ScaledDotProductAttention(512)\n",
    "\n",
    "        '''残差多尺度连接'''\n",
    "        self.fuse1 = MultiConv(768, 256)\n",
    "        self.fuse2 = MultiConv(384, 128)\n",
    "        self.fuse3 = MultiConv(192, 64)\n",
    "        self.fuse4 = MultiConv(128, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        '''Setting 1'''\n",
    "        x5_pam = self.pam(x5)\n",
    "\n",
    "        '''Setting 2'''\n",
    "        x5_pos = self.pos(x5)\n",
    "        x5 = x5 + x5_pos\n",
    "\n",
    "        x5_sdpa = self.sdpa(x5)\n",
    "        x5 = x5_sdpa + x5_pam\n",
    "\n",
    "        x6 = self.up1(x5, x4)\n",
    "        x5_scale = F.interpolate(x5, size=x6.shape[2:], mode='bilinear', align_corners=True)\n",
    "        x6_cat = torch.cat((x5_scale, x6), 1)\n",
    "\n",
    "        x7 = self.up2(x6_cat, x3)\n",
    "        x6_scale = F.interpolate(x6, size=x7.shape[2:], mode='bilinear', align_corners=True)\n",
    "        x7_cat = torch.cat((x6_scale, x7), 1)\n",
    "\n",
    "        x8 = self.up3(x7_cat, x2)\n",
    "        x7_scale = F.interpolate(x7, size=x8.shape[2:], mode='bilinear', align_corners=True)\n",
    "        x8_cat = torch.cat((x7_scale, x8), 1)\n",
    "\n",
    "        x9 = self.up4(x8_cat, x1)\n",
    "        x8_scale = F.interpolate(x8, size=x9.shape[2:], mode='bilinear', align_corners=True)\n",
    "        x9 = torch.cat((x8_scale, x9), 1)\n",
    "\n",
    "        logits = self.outc(x9)\n",
    "        return logits\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x = torch.randn(size=(1, 1, 128, 128, 128))\n",
    "    model = UNet_Attention_Transformer_Multiscale(1, 2, bilinear=True)\n",
    "    y = model(x)\n",
    "    print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5810cecb-7ab6-419b-aee1-eef30d309285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170.66666666666666"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512/3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5031b1b1-cfc0-4539-8a11-634c6c4aed70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "170+171+171"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90a1ad9-8489-4a52-99a4-aee34a226f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
